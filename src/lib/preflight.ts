/**
 * This file is used to preprocess the content of the site and update Algolia index. The site will use the
 * metadata.ts file generated by this file to render.
 */

import 'dotenv/config'

import {existsSync, readdirSync} from 'node:fs'
import {readFile, writeFile} from 'node:fs/promises'
import {basename} from 'node:path'
import {bundleMDX} from 'mdx-bundler'
import type {BundleMDXOptions} from 'mdx-bundler/dist/types'
import rehypeHighlight from 'rehype-highlight'
import rehypeKatex from 'rehype-katex'
import remarkGfm from 'remark-gfm'
import remarkMath from 'remark-math'
import remarkParse from 'remark-parse'
import remarkStringify from 'remark-stringify'
import remarkWikiLink from 'remark-wiki-link'
import stripMarkdown from 'strip-markdown'
import {unified} from 'unified'
import {SKIP, visit} from 'unist-util-visit'

import {generateEmbeddings, generateSummary} from '@/lib/language-models'

const HASHTAG_REGEX = /(?<![\w/])#(\w+)(?![\w/#])/g

type Frontmatter = {
  title?: string
  aliases?: string[]
  date?: string
  'date modified'?: string
}
type NoteMetadata = {
  id: string
  frontmatter: Frontmatter
  code: string
  plainText: string
  summary: string
  embedding: number[]
}
export type Metadata = {
  notes: Partial<Record<string, NoteMetadata>>
  hashtags: Partial<Record<string, string[]>>
  wikilinks: {from: string; to: string}[]
}
const preprocessMdx = async () => {
  const directory = `${process.cwd()}/content`
  if (!existsSync(directory)) throw new Error('Content directory not found')
  const filenames = readdirSync(directory).filter((filename) => filename.endsWith('.md'))
  const ids = new Set(filenames.map((filename) => basename(filename, '.md')))
  const entries: [string, NoteMetadata][] = []
  const hashtagToIds: Record<string, Set<string>> = {}
  const idToWikilinks: Record<string, Set<string>> = {}
  const mdxOptions = getMdxOptions([...ids])
  for (const [index, filename] of filenames.entries()) {
    console.log(`Processing ${index + 1}/${filenames.length} ${filename}`)
    const id = basename(filename, '.md')
    const rawContent = await readFile(`${directory}/${filename}`, 'utf8')
    const {code, frontmatter, matter} = await bundleMDX({
      source: rawContent,
      mdxOptions,
      esbuildOptions(options) {
        options.define = {
          'process.env.NODE_ENV': '"development"',
        }
        return options
      },
    })
    const ast = parser.parse(rawContent)
    parser.runSync(ast)
    visit(ast, 'hashtag', (node) => {
      const hashtag = node.data.hProperties.value.toLowerCase()
      if (!hashtagToIds[hashtag]) {
        hashtagToIds[hashtag] = new Set()
      }
      hashtagToIds[hashtag].add(id)
    })
    visit(ast, 'wikiLink', (node) => {
      const wikilink = node.data.permalink
      if (!ids.has(wikilink))
        throw new Error(`${id} has a wikilink pointing to a non-existing note`)
      if (!idToWikilinks[id]) {
        idToWikilinks[id] = new Set()
      }
      idToWikilinks[id].add(wikilink)
    })

    const plainText = await cleanMarkdown(matter.content)
    const summary = await generateSummary(plainText)
    entries.push([id, {id, frontmatter, code, plainText, summary, embedding: []}])
  }
  await computeNoteEmbeddings(entries)
  const notes = Object.fromEntries(entries)
  const output: Metadata = {
    notes,
    hashtags: Object.fromEntries(
      Object.entries(hashtagToIds).map(([hashtag, ids]) => [hashtag, Array.from(ids)]),
    ),
    wikilinks: Object.entries(idToWikilinks).flatMap(([id, wikilinks]) =>
      Array.from(wikilinks).map((to) => ({from: id, to})),
    ),
  }
  validate(output)
  const outputPath = `${process.cwd()}/src/app/metadata.ts`
  await writeFile(
    outputPath,
    `import type {Metadata} from '@/lib/preflight'\nexport const metadata: Metadata = ${JSON.stringify(output)}`,
  )
}

const getMdxOptions: (_: string[]) => Parameters<typeof bundleMDX>[0]['mdxOptions'] =
  (noteIds: string[]) => (options) => {
    options.remarkPlugins = [
      ...(options.remarkPlugins ?? []),
      remarkMath,
      remarkGfm,
      getWikiLinkPlugin(noteIds.map((id) => id.replace('index', ''))),
      remarkHashtags,
    ]
    options.rehypePlugins = [...(options.rehypePlugins ?? []), rehypeKatex, rehypeHighlight]
    return options
  }

type Plugin = ReturnType<
  BundleMDXOptions<Record<string, string>>['mdxOptions']
>['remarkPlugins'][number]
const getWikiLinkPlugin = (noteIds: string[]) =>
  [
    remarkWikiLink,
    {
      aliasDivider: '|',
      hrefTemplate: (link: string) => `/${link}`,
      permalinks: noteIds,
      wikiLinkClassName: 'text-primary underline',
      newClassName: 'text-primary/50',
    },
  ] satisfies Plugin

const remarkHashtags: Plugin = () => {
  return (tree) => {
    visit(tree, 'text', (node, index, parent) => {
      const {value} = node
      const matches = value.match(HASHTAG_REGEX)
      if (matches) {
        const parts = []
        let lastIndex = 0
        for (const match of value.matchAll(HASHTAG_REGEX)) {
          const [fullMatch, hashtag] = match
          const matchIndex = match.index

          // Add text before the hashtag
          if (matchIndex > lastIndex) {
            parts.push({
              type: 'text',
              value: value.slice(lastIndex, matchIndex),
            })
          }

          // Add the hashtag node
          parts.push({
            type: 'hashtag',
            value: fullMatch,
            data: {
              hName: 'hashtag',
              hProperties: {value: hashtag},
            },
          })

          lastIndex = matchIndex + fullMatch.length
        }

        // Add any remaining text after the last hashtag
        if (lastIndex < value.length) {
          parts.push({
            type: 'text',
            value: value.slice(lastIndex),
          })
        }

        parent.children.splice(index, 1, ...parts)
        return [SKIP, index + parts.length - 1]
      }
    })
  }
}

/**
 * Remove markdown syntax
 */
const cleanMarkdown = async (content: string) => {
  const processor = unified()
    .use(remarkParse)
    .use(remarkStringify)
    .use(stripMarkdown, {remove: ['table', 'math', 'inlineMath']})
  const plainText = await processor.process(content)
  return plainText.toString()
}

const parser = unified()
  .use(remarkParse)
  .use(...getWikiLinkPlugin([]))
  .use(remarkHashtags)

const validate = (data: Metadata) => {
  const ids = new Set(Object.keys(data.notes))
  const linkedIds = new Set([
    ...data.wikilinks.map(({from}) => from),
    ...data.wikilinks.map(({to}) => to),
  ])
  const orphanIds = Array.from(ids).filter((id) => !linkedIds.has(id))
  if (orphanIds.length > 0) {
    console.error(`${orphanIds.length} orphaned notes: ${[...orphanIds].join(', ')}`)
  }
}

const computeNoteEmbeddings = async (entries: [string, NoteMetadata][]) => {
  const embeddings = await generateEmbeddings(entries.map(([_, value]) => value.plainText))
  for (const [i, [_, value]] of entries.entries()) {
    value.embedding = embeddings[i]
  }
}

preprocessMdx()
